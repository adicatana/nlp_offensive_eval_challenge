{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task_B_full_training.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "udvQ3c7aGX1g",
        "colab_type": "code",
        "outputId": "da987643-7f2a-4708-98fb-b37c24eabe23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        "# Loading packages\n",
        "!pip install nltk\n",
        "\n",
        "exec(open('utilities_cw.py').read())\n",
        "\n",
        "# We fix the seeds to get consistent results\n",
        "SEED = 1111\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "one.py is being run directly\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xpPZHdI4HB34",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_file = 'offenseval-training-v1.tsv'\n",
        "corpus, labels = parse_dataset_task_b(train_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e9msXLzzYRdk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(len(train_corpus))\n",
        "print(len(train_labels))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LXpGXf8AY7X_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_file = 'validation.tsv'\n",
        "test_corpus, test_labels = parse_dataset_task_b(test_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wyhvMq7pY_hk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "corpus += test_corpus\n",
        "labels += test_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A_ll6QLjYVSf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_corpus, valid_corpus, train_labels, valid_labels = train_test_split(corpus, labels, test_size=0, random_state=42)\n",
        "\n",
        "# train_corpus, train_labels = augment_untargeted(train_corpus, train_labels, 1600)\n",
        "# valid_corpus, valid_labels = augment_untargeted(valid_corpus, valid_labels, 900)\n",
        "    \n",
        "print(train_corpus)\n",
        "print(train_labels)\n",
        "\n",
        "print(len(train_corpus))\n",
        "print(len(train_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E_CgPYWoZM3_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenize_f = tokenize_stemming"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UnM24ihrKdaS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenized_train_corpus = tokenize_f(train_corpus)\n",
        "print(len(tokenized_train_corpus))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WPVTr3zmMfpD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocabulary = get_vocabulary(tokenized_train_corpus)\n",
        "print(vocabulary)\n",
        "print(len(vocabulary))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "14T5h_aaMBY_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word2idx = get_word2idx(tokenized_train_corpus, vocabulary)\n",
        "print(len(word2idx))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nenApEbLNTih",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "idx2word = get_idx2word(vocabulary)\n",
        "print(len(idx2word))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EPUENzKZNiEU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentences_lengths = [len(sentence) for sentence in tokenized_train_corpus]\n",
        "max_len = np.max(np.array(sentences_lengths))\n",
        "\n",
        "train_sentences_tensor, train_labels_tensor = parse_input(tokenized_train_corpus, word2idx, train_labels, max_len)\n",
        "\n",
        "print(train_sentences_tensor)\n",
        "print(train_labels_tensor)\n",
        "\n",
        "print(train_sentences_tensor.shape)\n",
        "print(train_labels_tensor.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7IWSCEJ-uRAD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def parse_in(tokenized_corpus, word2idx, labels, max_len):\n",
        "    # we index our sentences\n",
        "    vectorized_sentences = [[word2idx[token] for token in sentence if token in word2idx] for sentence in tokenized_corpus]\n",
        "  \n",
        "  \n",
        "    # we create a tensor of a fixed size filled with zeroes for padding\n",
        "    sentences_tensor = Variable(torch.zeros((len(vectorized_sentences), max_len))).long()\n",
        "    sentences_lengths = [len(sentence) for sentence in vectorized_sentences]\n",
        "\n",
        "    # we fill it with our vectorized sentences \n",
        "    for idx, (sentence, sentence_len) in enumerate(zip(vectorized_sentences, sentences_lengths)):\n",
        "        sentences_tensor[idx, :sentence_len] = torch.LongTensor(sentence)\n",
        "            \n",
        "    labels_tensor = torch.FloatTensor(labels)\n",
        "\n",
        "    return sentences_tensor, labels_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c9GPEYBZrTys",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model_custom(model, optimizer, loss_fn, feature_train, target_train):    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(feature_train).squeeze(1)\n",
        "        loss = loss_fn(predictions, target_train)\n",
        "        acc = accuracy(predictions, target_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss = loss.item()\n",
        "        epoch_acc = acc\n",
        "  \n",
        "        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.3f} | Train Acc: {epoch_acc*100:.2f}% | ')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PTTWtDn1PNNw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs=100\n",
        "\n",
        "INPUT_DIM = len(word2idx)\n",
        "EMBEDDING_DIM = 100\n",
        "OUTPUT_DIM = 1\n",
        "LEARNING_RATE = 0.0008\n",
        "\n",
        "# the hyperparamerts specific to CNN\n",
        "# we define the number of filters\n",
        "N_OUT_CHANNELS = 100\n",
        "# we define the window size\n",
        "WINDOW_SIZE = 2\n",
        "# we apply the dropout with the probability 0.5\n",
        "DROPOUT = 0.5\n",
        "\n",
        "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_OUT_CHANNELS, WINDOW_SIZE, OUTPUT_DIM, DROPOUT)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "train_model_custom(model, optimizer, loss_fn, train_sentences_tensor, train_labels_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EvrvSQtc9dX5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "testing = read_csv('testset-taskb.tsv')\n",
        "\n",
        "tweets = [row['tweet'] for row in testing]\n",
        "tokenized_corpus = tokenize(tweets)\n",
        "\n",
        "test_tensor, labels_tensor = parse_input(tokenized_corpus, word2idx, test_labels, max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jbdIL00w-Tm3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions_test = model(test_tensor).squeeze(1)\n",
        "\n",
        "output = torch.round(torch.sigmoid(predictions_test))\n",
        "\n",
        "print(output)\n",
        "\n",
        "print(len(output))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M60hnPOPHfT4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ids = [row['id'] for row in testing]\n",
        "out = ['UNT' if e == 0 else 'TIN' for e in output.detach().numpy()]\n",
        "zipped = list(zip(ids, out))\n",
        "\n",
        "with open('predictions_task_b_full_training_stemming_window_2.csv', \"w\") as f:\n",
        "    writer = csv.writer(f, dialect='excel')\n",
        "    for row in zipped:\n",
        "        writer.writerow(row)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jhNmdC8eIo0X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}